


import os

# change to your GitHub repo folder on D:
os.chdir(f"D:\Github Docs\SF-Budget-Intelligence-Forecasting-Compliance-and-Insights")

# confirm
print("Current working directory:", os.getcwd())



import pandas as pd
pd.set_option('display.max_columns', None)


import pandas as pd

# The original dataset (~140MB) was too large for GitHub.
# It has been split into two parts: Budget_20250921-1.csv and Budget_20250921-2.csv
# We combine them here into a single DataFrame for analysis.

df1 = pd.read_csv("data/raw/Budget_20250921-1.csv")
df2 = pd.read_csv("data/raw/Budget_20250921-2.csv")

# Concatenate the two halves into one DataFrame
df = pd.concat([df1, df2], ignore_index=True)

# Quick check of the combined dataset
df.head()



# Shape of the dataset
print("Rows:", df.shape[0], "Columns:", df.shape[1])

# Data types and non-null counts
df.info()



# Count missing values per column
df.isna().sum()



# Total duplicate rows
print("Duplicate rows:", df.duplicated().sum())

# Check duplicates by key fields (adjust based on dictionary)
dup_keys = df.duplicated(subset=["Fiscal Year", "Department", "Fund", "Program"])
print("Duplicates by Fiscal Year + Department + Fund + Program:", dup_keys.sum())



# Summary statistics for numeric-like columns
df.describe(include="all").T






